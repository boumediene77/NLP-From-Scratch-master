{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading datasets...\n",
      "INFO:root:Vocabulary size: 42928\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datamaestro import prepare_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import datetime\n",
    "from typing import List\n",
    "import time\n",
    "from pathlib import Path\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "ds = prepare_dataset('org.universaldependencies.french.gsd')\n",
    "\n",
    "\n",
    "# Format de sortie décrit dans\n",
    "# https://pypi.org/project/conllu/\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Permet de gérer un vocabulaire.\n",
    "\n",
    "    En test, il est possible qu'un mot ne soit pas dans le\n",
    "    vocabulaire : dans ce cas le token \"__OOV__\" est utilisé.\n",
    "    Attention : il faut tenir compte de cela lors de l'apprentissage !\n",
    "\n",
    "    Utilisation:\n",
    "\n",
    "    - en train, utiliser v.get(\"blah\", adding=True) pour que le mot soit ajouté\n",
    "      automatiquement s'il n'est pas connu\n",
    "    - en test, utiliser v[\"blah\"] pour récupérer l'ID du mot (ou l'ID de OOV)\n",
    "    \"\"\"\n",
    "    OOVID = 1\n",
    "    PAD = 0\n",
    "\n",
    "    def __init__(self, oov: bool):\n",
    "        \"\"\" oov : autorise ou non les mots OOV \"\"\"\n",
    "        self.oov =  oov\n",
    "        self.id2word = [ \"PAD\"]\n",
    "        self.word2id = { \"PAD\" : Vocabulary.PAD}\n",
    "        if oov:\n",
    "            self.word2id[\"__OOV__\"] = Vocabulary.OOVID\n",
    "            self.id2word.append(\"__OOV__\")\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if self.oov:\n",
    "            return self.word2id.get(word, Vocabulary.OOVID)\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def get(self, word: str, adding=True):\n",
    "        try:\n",
    "            return self.word2id[word]\n",
    "        except KeyError:\n",
    "            if adding:\n",
    "                wordid = len(self.id2word)\n",
    "                self.word2id[word] = wordid\n",
    "                self.id2word.append(word)\n",
    "                return wordid\n",
    "            if self.oov:\n",
    "                return Vocabulary.OOVID\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id2word)\n",
    "\n",
    "    def getword(self,idx: int):\n",
    "        if idx < len(self):\n",
    "            return self.id2word[idx]\n",
    "        return None\n",
    "\n",
    "    def getwords(self,idx: List[int]):\n",
    "        return [self.getword(i) for i in idx]\n",
    "\n",
    "\n",
    "\n",
    "class TaggingDataset():\n",
    "    def __init__(self, data, words: Vocabulary, tags: Vocabulary, adding=True):\n",
    "        self.sentences = []\n",
    "\n",
    "        for s in data:\n",
    "            self.sentences.append(([words.get(token[\"form\"], adding) for token in s], [tags.get(token[\"upostag\"], adding) for token in s]))\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    def __getitem__(self, ix):\n",
    "        return self.sentences[ix]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate using pad_sequence\"\"\"\n",
    "    return tuple(pad_sequence([torch.LongTensor(b[j]) for b in batch]) for j in range(2))\n",
    "\n",
    "\n",
    "logging.info(\"Loading datasets...\")\n",
    "words = Vocabulary(True)\n",
    "tags = Vocabulary(False)\n",
    "train_data = TaggingDataset(ds.train, words, tags, True)\n",
    "dev_data = TaggingDataset(ds.validation, words, tags, True)\n",
    "test_data = TaggingDataset(ds.test, words, tags, False)\n",
    "\n",
    "\n",
    "logging.info(\"Vocabulary size: %d\", len(words))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "\n",
    "train_loader = DataLoader(train_data, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, collate_fn=collate_fn, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, collate_fn=collate_fn, batch_size=BATCH_SIZE)\n",
    "\n",
    "EMB_DIM = len(words) // 100\n",
    "HIDDEN_DIM = 10\n",
    "VOCAB_SIZE = len(words)\n",
    "TAGSET_SIZE = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer = SummaryWriter(\"runs/tagger/runs\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "class State:\n",
    "    def __init__(self, model, optim):\n",
    "        self.model = model\n",
    "        self.optimizer = optim\n",
    "        self.epoch, self.iteration = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader,state):\n",
    "    train_loss = 0\n",
    "    nb_oov = 10\n",
    "    for batch, (X, y) in enumerate(train_loader): \n",
    "        #OOV\n",
    "        i, j = np.random.randint(0, X.shape[0], size=nb_oov), np.random.randint(0, X.shape[1], size=nb_oov)\n",
    "        for k,l in zip(i,j):\n",
    "            if X[k, l] != 0: # On remplace pas les pads!\n",
    "                X[k, l] = 1\n",
    "        yhat = state.model(X)\n",
    "        L = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        y = y.permute(1, 0)\n",
    "        yhat = yhat.permute(1, 2, 0)\n",
    "        loss = L(yhat , y)\n",
    "        state.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        state.optimizer.step()\n",
    "        state.iteration += 1\n",
    "        train_loss += loss\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    return train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader,model):\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(train_loader): \n",
    "            yhat = model(X)\n",
    "            L = nn.CrossEntropyLoss(ignore_index=0)\n",
    "            y = y.permute(1, 0)\n",
    "            yhat = yhat.permute(1, 2, 0)\n",
    "            loss = L(yhat , y)\n",
    "            test_loss += loss\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    return test_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_train, data_test, save_path, Model, tensorboard_name, iterations=500):\n",
    "    if save_path.is_file():\n",
    "        with save_path.open('rb') as fp:\n",
    "            state = torch.load(fp)\n",
    "    else :\n",
    "        model = Model(EMB_DIM, HIDDEN_DIM, VOCAB_SIZE, TAGSET_SIZE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        state = State(model, optimizer)\n",
    "    for epoch in range(state.epoch, iterations):\n",
    "        loss_train = train_loop(data_train, state)\n",
    "        with save_path.open(\"wb\") as fp:\n",
    "            state.epoch = epoch + 1\n",
    "            torch.save(state, fp)\n",
    "        loss_test = test_loop(data_test, state.model)\n",
    "        writer.add_scalar(tensorboard_name+'/train', loss_train, epoch)\n",
    "        writer.add_scalar(tensorboard_name+'/dev',loss_test , epoch)\n",
    "        print('Epoch: ', epoch, ' Loss dev: ', loss_test, 'Loss train: ',loss_train)\n",
    "    print(\"Done!\")\n",
    "    return state.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss dev:  57.150394439697266 Loss train:  2.27164626121521\n",
      "Epoch:  1  Loss dev:  38.48114776611328 Loss train:  1.4752479791641235\n",
      "Epoch:  2  Loss dev:  27.564605712890625 Loss train:  1.0282931327819824\n",
      "Epoch:  3  Loss dev:  20.744081497192383 Loss train:  0.7607361674308777\n",
      "Epoch:  4  Loss dev:  16.40595245361328 Loss train:  0.5908533930778503\n",
      "Epoch:  5  Loss dev:  13.465606689453125 Loss train:  0.47762659192085266\n",
      "Epoch:  6  Loss dev:  11.350252151489258 Loss train:  0.39774686098098755\n",
      "Epoch:  7  Loss dev:  9.81848430633545 Loss train:  0.34065738320350647\n",
      "Epoch:  8  Loss dev:  8.62510871887207 Loss train:  0.29790744185447693\n",
      "Epoch:  9  Loss dev:  7.686396598815918 Loss train:  0.26438474655151367\n",
      "Epoch:  10  Loss dev:  6.938488960266113 Loss train:  0.23740534484386444\n",
      "Epoch:  11  Loss dev:  6.31382417678833 Loss train:  0.2160046547651291\n",
      "Epoch:  12  Loss dev:  5.804275989532471 Loss train:  0.197427898645401\n",
      "Epoch:  13  Loss dev:  5.3275837898254395 Loss train:  0.1823277771472931\n",
      "Epoch:  14  Loss dev:  4.946291446685791 Loss train:  0.16910959780216217\n",
      "Epoch:  15  Loss dev:  4.626342296600342 Loss train:  0.15807917714118958\n",
      "Epoch:  16  Loss dev:  4.337149620056152 Loss train:  0.14814724028110504\n",
      "Epoch:  17  Loss dev:  4.105727672576904 Loss train:  0.13948258757591248\n",
      "Epoch:  18  Loss dev:  3.8856561183929443 Loss train:  0.1325473189353943\n",
      "Epoch:  19  Loss dev:  3.691063404083252 Loss train:  0.12617364525794983\n",
      "Epoch:  20  Loss dev:  3.5279083251953125 Loss train:  0.12084382027387619\n",
      "Epoch:  21  Loss dev:  3.380788803100586 Loss train:  0.115696020424366\n",
      "Epoch:  22  Loss dev:  3.2320096492767334 Loss train:  0.11068962514400482\n",
      "Epoch:  23  Loss dev:  3.091719388961792 Loss train:  0.10645027458667755\n",
      "Epoch:  24  Loss dev:  2.9726147651672363 Loss train:  0.10224130004644394\n",
      "Epoch:  25  Loss dev:  2.8844640254974365 Loss train:  0.09890313446521759\n",
      "Epoch:  26  Loss dev:  2.762028455734253 Loss train:  0.09588150680065155\n",
      "Epoch:  27  Loss dev:  2.6763720512390137 Loss train:  0.09341195970773697\n",
      "Epoch:  28  Loss dev:  2.5990748405456543 Loss train:  0.09015025198459625\n",
      "Epoch:  29  Loss dev:  2.513760805130005 Loss train:  0.08788097649812698\n",
      "Epoch:  30  Loss dev:  2.451374053955078 Loss train:  0.0851871520280838\n",
      "Epoch:  31  Loss dev:  2.3965303897857666 Loss train:  0.08310163766145706\n",
      "Epoch:  32  Loss dev:  2.3233606815338135 Loss train:  0.08124260604381561\n",
      "Epoch:  33  Loss dev:  2.260270833969116 Loss train:  0.07930798083543777\n",
      "Epoch:  34  Loss dev:  2.2391438484191895 Loss train:  0.07722460478544235\n",
      "Epoch:  35  Loss dev:  2.1761579513549805 Loss train:  0.07609916478395462\n",
      "Epoch:  36  Loss dev:  2.1196112632751465 Loss train:  0.07420538365840912\n",
      "Epoch:  37  Loss dev:  2.067485809326172 Loss train:  0.072878897190094\n",
      "Epoch:  38  Loss dev:  2.0304341316223145 Loss train:  0.07112805545330048\n",
      "Epoch:  39  Loss dev:  1.9803459644317627 Loss train:  0.06985240429639816\n",
      "Epoch:  40  Loss dev:  1.967138648033142 Loss train:  0.06888798624277115\n",
      "Epoch:  41  Loss dev:  1.9198249578475952 Loss train:  0.06782171875238419\n",
      "Epoch:  42  Loss dev:  1.8973290920257568 Loss train:  0.0665961429476738\n",
      "Epoch:  43  Loss dev:  1.839626431465149 Loss train:  0.06588208675384521\n",
      "Epoch:  44  Loss dev:  1.8287731409072876 Loss train:  0.0645679235458374\n",
      "Epoch:  45  Loss dev:  1.7863410711288452 Loss train:  0.06361619383096695\n",
      "Epoch:  46  Loss dev:  1.7615654468536377 Loss train:  0.06311942636966705\n",
      "Epoch:  47  Loss dev:  1.7227678298950195 Loss train:  0.062089093029499054\n",
      "Epoch:  48  Loss dev:  1.694549560546875 Loss train:  0.06099991500377655\n",
      "Epoch:  49  Loss dev:  1.6631877422332764 Loss train:  0.060077965259552\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "savepath = Path('./lstmTagger.pt')\n",
    "model = train(train_loader, dev_loader, savepath, LSTMTagger, \"LSTMTagger\", iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calculator(model, loader):\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, y) in enumerate(loader): \n",
    "            yhat = model(X)\n",
    "            L = nn.CrossEntropyLoss()\n",
    "            y = y.permute(1, 0)\n",
    "            yhat = yhat.permute(1, 2, 0)\n",
    "            loss = L(yhat , y)\n",
    "            _, pred = torch.max(yhat, 1)\n",
    "            partial_acc = 0\n",
    "            for (i, y_row) in enumerate(y):\n",
    "                y_filtered = y_row[y_row != 0]\n",
    "                pred_filtered = pred[i][:len(y_filtered)]\n",
    "                partial_acc += torch.sum( pred_filtered == y_filtered) / len(y_filtered)\n",
    "            acc = partial_acc / loader.batch_size\n",
    "            test_acc += acc\n",
    "            test_loss += loss\n",
    "    return test_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8573)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_calculator(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8430)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_calculator(model, dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9845)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_calculator(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:La                   Tag:DET                  Predicted Tag:DET                 \n",
      "Word:__OOV__              Tag:NOUN                 Predicted Tag:NOUN                \n",
      "Word:rend                 Tag:VERB                 Predicted Tag:VERB                \n",
      "Word:aussi                Tag:ADV                  Predicted Tag:ADV                 \n",
      "Word:__OOV__              Tag:ADJ                  Predicted Tag:DET                 \n",
      "Word:que                  Tag:SCONJ                Predicted Tag:SCONJ               \n",
      "Word:les                  Tag:DET                  Predicted Tag:DET                 \n",
      "Word:drogues              Tag:NOUN                 Predicted Tag:NOUN                \n",
      "Word:dures                Tag:ADJ                  Predicted Tag:ADJ                 \n",
      "Word:!                    Tag:PUNCT                Predicted Tag:PUNCT               \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch, (X, y) in enumerate(test_loader): \n",
    "        \n",
    "        yhat = model(X)\n",
    "        L = nn.CrossEntropyLoss()\n",
    "        y = y.permute(1, 0)\n",
    "        yhat = yhat.permute(1, 2, 0)\n",
    "        loss = L(yhat , y)\n",
    "        _, pred = torch.max(yhat, 1)\n",
    "        acc = torch.sum( pred == y) / (test_loader.batch_size * y.shape[1])\n",
    "        break\n",
    "i = 0\n",
    "s = 55\n",
    "X = X.T\n",
    "wds = words.getwords(X[s,:])\n",
    "tgs = tags.getwords(y[s,:])\n",
    "predtgs = tags.getwords(pred[s,:])\n",
    "while wds[i] != 'PAD':\n",
    "    print(\"Word:{: <20} Tag:{: <20} Predicted Tag:{: <20}\"\\\n",
    "          .format(wds[i], tgs[i], predtgs[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
