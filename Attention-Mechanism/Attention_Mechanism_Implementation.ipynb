{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -r http://webia.lip6.fr/~baskiotisn/requirements-amal.txt","metadata":{"id":"8isSQNF0Vjw2","outputId":"7dd3a7b6-8bc4-48b6-d3f6-4d0c76be418b","execution":{"iopub.status.busy":"2022-01-23T10:36:48.066208Z","iopub.execute_input":"2022-01-23T10:36:48.066826Z","iopub.status.idle":"2022-01-23T10:37:48.814149Z","shell.execute_reply.started":"2022-01-23T10:36:48.066733Z","shell.execute_reply":"2022-01-23T10:37:48.812879Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nimport re\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\nfrom datamaestro import prepare_dataset\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"id":"W5p5YRJoVjw7","execution":{"iopub.status.busy":"2022-01-23T10:38:10.328921Z","iopub.execute_input":"2022-01-23T10:38:10.329294Z","iopub.status.idle":"2022-01-23T10:38:12.336380Z","shell.execute_reply.started":"2022-01-23T10:38:10.329203Z","shell.execute_reply":"2022-01-23T10:38:12.335182Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class FolderText(Dataset):\n    \"\"\"Dataset basé sur des dossiers (un par classe) et fichiers\"\"\"\n\n    def __init__(self, classes, folder: Path, tokenizer, load=False):\n        self.tokenizer = tokenizer\n        self.files = []\n        self.filelabels = []\n        self.labels = {}\n        for ix, key in enumerate(classes):\n            self.labels[key] = ix\n\n        for label in classes:\n            for file in (folder / label).glob(\"*.txt\"):\n                self.files.append(file.read_text(encoding='cp437') if load else file)\n                self.filelabels.append(self.labels[label])\n\n    def __len__(self):\n        return len(self.filelabels)\n\n    def __getitem__(self, ix):\n        s = self.files[ix]\n        return self.tokenizer(s if isinstance(s, str) else s.read_text()), self.filelabels[ix]","metadata":{"id":"xShOHe_ZVjw8","execution":{"iopub.status.busy":"2022-01-23T10:38:13.742409Z","iopub.execute_input":"2022-01-23T10:38:13.742717Z","iopub.status.idle":"2022-01-23T10:38:13.754448Z","shell.execute_reply.started":"2022-01-23T10:38:13.742684Z","shell.execute_reply":"2022-01-23T10:38:13.751837Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_imdb_data(embedding_size=50):\n    \"\"\"Renvoie l'ensemble des donnéees nécessaires pour l'apprentissage\n\n    - dictionnaire word vers ID\n    - embeddings (Glove)\n    - DataSet (FolderText)\n\n    \"\"\"\n    WORDS = re.compile(r\"\\S+\")\n\n    words, embeddings = prepare_dataset('edu.stanford.glove.6b.%d' % embedding_size).load()\n    OOVID = len(words)\n    words.append(\"__OOV__\")\n    words.append(\"__PAD__\")\n    \n    word2id = {word: ix for ix, word in enumerate(words)}\n    embeddings = np.vstack((embeddings, np.zeros(embedding_size), np.ones(embedding_size)))\n\n    def tokenizer(t):\n        return [word2id.get(x, OOVID) for x in re.findall(WORDS, t.lower())]\n\n    logging.info(\"Loading embeddings\")\n\n    logging.info(\"Get the IMDB dataset\")\n    ds = prepare_dataset(\"edu.stanford.aclimdb\")\n\n    return word2id, embeddings, FolderText(ds.train.classes, ds.train.path, tokenizer, load=False), FolderText(ds.test.classes, ds.test.path, tokenizer, load=False)\n\n","metadata":{"id":"juhyZVp1Vjw9","execution":{"iopub.status.busy":"2022-01-23T10:38:14.105414Z","iopub.execute_input":"2022-01-23T10:38:14.106259Z","iopub.status.idle":"2022-01-23T10:38:14.117216Z","shell.execute_reply.started":"2022-01-23T10:38:14.106204Z","shell.execute_reply":"2022-01-23T10:38:14.116010Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"EMB_DIM = 200\nBATCH_SIZE = 64\nMAX_LENGTH = 500","metadata":{"id":"D2dX7fb9Vjw-","execution":{"iopub.status.busy":"2022-01-23T10:38:15.451201Z","iopub.execute_input":"2022-01-23T10:38:15.451499Z","iopub.status.idle":"2022-01-23T10:38:15.456607Z","shell.execute_reply.started":"2022-01-23T10:38:15.451466Z","shell.execute_reply":"2022-01-23T10:38:15.455260Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"word2id, embeddings, data_train, data_test = get_imdb_data(embedding_size=EMB_DIM)","metadata":{"id":"yinulEj7Vjw9","outputId":"8acffee5-3a24-4331-a50f-09f2014d9630","execution":{"iopub.status.busy":"2022-01-23T10:38:15.810967Z","iopub.execute_input":"2022-01-23T10:38:15.811634Z","iopub.status.idle":"2022-01-23T10:42:59.627082Z","shell.execute_reply.started":"2022-01-23T10:38:15.811595Z","shell.execute_reply":"2022-01-23T10:42:59.625533Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nemb_layer = nn.Embedding.from_pretrained(torch.Tensor(embeddings))\ndef collate_fn(batch):\n    \"\"\"Collate using pad_sequence\"\"\"\n    data = [torch.LongTensor(item[0][:MAX_LENGTH]) for item in batch]\n    labels = [b[1] for b in batch]\n    return (emb_layer(pad_sequence(data, batch_first=True,padding_value = word2id[\"__PAD__\"])).to(device), \n                 torch.LongTensor(labels).to(device))","metadata":{"id":"7Rz7CndfVjw-","execution":{"iopub.status.busy":"2022-01-23T10:42:59.635176Z","iopub.execute_input":"2022-01-23T10:42:59.638552Z","iopub.status.idle":"2022-01-23T10:42:59.896862Z","shell.execute_reply.started":"2022-01-23T10:42:59.638445Z","shell.execute_reply":"2022-01-23T10:42:59.895814Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(data_train, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(data_test, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"id":"x-F0jKspVjw_","execution":{"iopub.status.busy":"2022-01-23T10:43:19.698645Z","iopub.execute_input":"2022-01-23T10:43:19.698957Z","iopub.status.idle":"2022-01-23T10:43:19.706672Z","shell.execute_reply.started":"2022-01-23T10:43:19.698910Z","shell.execute_reply":"2022-01-23T10:43:19.705046Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class BaselineModel(nn.Module):\n    def __init__(self, emb_dim):\n        super(BaselineModel, self).__init__()\n        self.emb_dim = emb_dim\n        self.linear = nn.Linear(emb_dim, 1)\n        self.m = nn.Sigmoid()\n    \n    def forward(self, x):\n        mask0 = torch.zeros_like(x)\n        mask1 = torch.ones_like(x)\n        mask = torch.where(x!=0,mask1,mask0)\n        x = torch.sum(x,dim=1)/torch.sum(mask,dim=1)\n        x = self.linear(x)\n        return self.m(x)","metadata":{"id":"4NbDVJ1dVjxA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class State:\n    def __init__(self, model, optim):\n        self.model = model\n        self.optimizer = optim\n        self.epoch, self.iteration = 0, 0","metadata":{"id":"Uv3Z0AIKVjxA","execution":{"iopub.status.busy":"2022-01-23T10:43:50.582895Z","iopub.execute_input":"2022-01-23T10:43:50.583209Z","iopub.status.idle":"2022-01-23T10:43:50.591066Z","shell.execute_reply.started":"2022-01-23T10:43:50.583176Z","shell.execute_reply":"2022-01-23T10:43:50.589375Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, state):\n    train_loss = 0\n    train_acc = 0\n    L = nn.BCELoss()\n    for batch, (X, y) in enumerate(dataloader):\n        #X = X.permute(1,0,2)    \n        yhat = state.model(X)\n        y = y.reshape(-1, 1).float()\n        loss = L(yhat, y)\n        state.optimizer.zero_grad()\n        loss.backward()\n        state.optimizer.step()\n        state.iteration += 1\n        acc = (torch.sum( torch.where(yhat > 0.5, 1, 0) == y) / dataloader.batch_size)\n        train_acc += acc\n        train_loss += loss\n\n    train_acc = train_acc / len(dataloader)\n    train_loss = train_loss / len(dataloader)\n    return train_loss.item(), train_acc.item()","metadata":{"id":"iyL65vDhVjxB","execution":{"iopub.status.busy":"2022-01-23T10:43:23.512396Z","iopub.execute_input":"2022-01-23T10:43:23.513249Z","iopub.status.idle":"2022-01-23T10:43:23.522364Z","shell.execute_reply.started":"2022-01-23T10:43:23.513209Z","shell.execute_reply":"2022-01-23T10:43:23.520763Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def test_loop(dataloader, model):\n    test_loss = 0\n    test_accuracy = 0\n    L = nn.BCELoss()\n    for batch, (X, y) in enumerate(dataloader):  \n        with torch.no_grad():\n            #X = X.permute(1,0,2)    \n            yhat = model(X)\n            y = y.reshape(-1, 1).float()\n            loss = L(yhat, y)\n            test_loss += loss\n            acc = (torch.sum( torch.where(yhat > 0.5, 1, 0) == y) / dataloader.batch_size)\n            test_accuracy += acc\n\n    test_loss = test_loss / len(dataloader)\n    test_accuracy = test_accuracy / len(dataloader)\n    return test_loss.item(), test_accuracy.item()","metadata":{"id":"skEgzU26eZU4","execution":{"iopub.status.busy":"2022-01-23T10:43:23.887421Z","iopub.execute_input":"2022-01-23T10:43:23.888507Z","iopub.status.idle":"2022-01-23T10:43:23.898545Z","shell.execute_reply.started":"2022-01-23T10:43:23.888452Z","shell.execute_reply":"2022-01-23T10:43:23.897206Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def train(data_train, data_test, save_path, Model, tensorboard_name, iterations=500):\n    if save_path.is_file():\n        with save_path.open('rb') as fp:\n            state = torch.load(fp)\n    else :\n        model = Model(EMB_DIM).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n        state = State(model, optimizer)\n    for epoch in range(state.epoch, iterations):\n        loss_train, acc_train = train_loop(data_train, state)\n        loss_test, acc_test = test_loop(data_test, state.model)\n        with save_path.open(\"wb\") as fp:\n            state.epoch = epoch + 1\n            torch.save(state, fp)\n        #loss_test = test_loop(data_test, state.model)\n        #writer.add_scalar(tensorboard_name+'/train', loss_test, epoch)\n        #writer.add_scalar(tensorboard_name+'/dev', loss_train, epoch)\n        print('Epoch: ', epoch, 'Loss train: ',loss_train, 'Acc train: ',acc_train)\n        print('       ', epoch, 'Loss test: ',loss_test, 'Acc test: ',acc_test)\n    print(\"Done!\")\n    return state.model","metadata":{"id":"nC6S2yqIVjxB","execution":{"iopub.status.busy":"2022-01-23T10:43:24.415934Z","iopub.execute_input":"2022-01-23T10:43:24.416529Z","iopub.status.idle":"2022-01-23T10:43:24.427711Z","shell.execute_reply.started":"2022-01-23T10:43:24.416474Z","shell.execute_reply":"2022-01-23T10:43:24.426278Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"savepath = Path('./BaselineModel.pt')\nmodel = train(train_loader, test_loader, savepath, BaselineModel, \"baseline\", iterations=20)","metadata":{"id":"aUKmSkpGVjxC","outputId":"f5165e2b-4256-416d-ab08-b84623843388"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SimpleAttention(nn.Module):\n    def __init__(self, emb_dim):\n        super(SimpleAttention, self).__init__()\n        self.emb_dim = emb_dim\n        self.q = nn.Linear(emb_dim, emb_dim)\n        self.linear = nn.Linear(emb_dim, 1)\n        self.m = nn.Sigmoid()\n    \n    def forward(self, x):\n        q = self.q(x)\n\n        q = torch.bmm(q, x.permute(0,2,1))\n\n        mask0 = torch.zeros_like(q)\n        q_masked = torch.where(q.float()==0, mask0-np.inf, q)\n        alpha = F.softmax(q_masked, dim=-1)\n\n        x = torch.bmm(alpha, x)\n       \n        x = x.sum(dim=1)\n        \n        x = self.linear(x)\n        \n        return self.m(x)","metadata":{"id":"d4FGcHuUVjxC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"savepath = Path('./SimpleAttention.pt')\nmodel = train(train_loader, test_loader, savepath, SimpleAttention, \"SimpleAttention\", iterations=20)","metadata":{"id":"4d1faeIVCD-K","outputId":"11abb54e-01a1-4d96-875a-02ef914dd8cf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class QVAttention(nn.Module):\n    def __init__(self, emb_dim):\n        super(QVAttention, self).__init__()\n        self.emb_dim = emb_dim\n        self.q = nn.Linear(emb_dim, emb_dim)\n        self.linear = nn.Linear(emb_dim, 1)\n        self.m = nn.Sigmoid()\n    \n    def forward(self, x):\n        \n        mask0 = torch.zeros_like(x)\n        mask1 = torch.ones_like(x)\n        mask = torch.where(x!=0,mask1,mask0)\n        tm = torch.sum(x,dim=1)/torch.sum(mask,dim=1)\n       \n        q = self.q(tm).unsqueeze(1)\n\n        q = torch.bmm(q, x.permute(0,2,1)) # les mots (x) sont les clés (k)\n\n        mask0 = torch.zeros_like(q)\n        q_masked = torch.where(q.float()==0, mask0-np.inf, q)\n        alpha = F.softmax(q_masked, dim=-1)\n\n        x = torch.bmm(alpha, x)\n       \n        x = x.sum(dim=1)\n        \n        x = self.linear(x)\n        \n        return self.m(x)","metadata":{"id":"IcUFCs2gEHMk","execution":{"iopub.status.busy":"2022-01-23T10:43:31.295001Z","iopub.execute_input":"2022-01-23T10:43:31.295375Z","iopub.status.idle":"2022-01-23T10:43:31.306086Z","shell.execute_reply.started":"2022-01-23T10:43:31.295341Z","shell.execute_reply":"2022-01-23T10:43:31.305012Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"savepath = Path('./QVAttention.pt')\nmodel = train(train_loader, test_loader, savepath, QVAttention, \"QVAttention\", iterations=20)","metadata":{"id":"USNsxCABfkuF","outputId":"cbc35b26-128d-4162-aaeb-93338d8ca345","execution":{"iopub.status.busy":"2022-01-23T10:43:54.365237Z","iopub.execute_input":"2022-01-23T10:43:54.365875Z","iopub.status.idle":"2022-01-23T10:48:59.774020Z","shell.execute_reply.started":"2022-01-23T10:43:54.365835Z","shell.execute_reply":"2022-01-23T10:48:59.772571Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"oIxqgCtcgJW7"},"execution_count":null,"outputs":[]}]}