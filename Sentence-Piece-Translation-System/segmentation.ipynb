{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "segmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNVjonserEqw",
        "outputId": "8d483dda-2425-436e-cecd-3193b6294bd2"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAr-H8nkUFXr"
      },
      "source": [
        "import sentencepiece as spm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm29823orCbo"
      },
      "source": [
        "spm.SentencePieceTrainer.train(\n",
        "    input = './en-fra.txt',\n",
        "    model_prefix = './model',\n",
        "    vocab_size=1000,\n",
        "    user_defined_symbols = []\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6rDcjo9vVus"
      },
      "source": [
        "a = spm.SentencePieceProcessor(model_file='model.model')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07ZC1oJRwnQP",
        "outputId": "552a3dd3-bbd9-47ac-a2f1-3e1a32b803cd"
      },
      "source": [
        "ids = a.encode(\"do you want a car\", out_type=int)\n",
        "ids"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[89, 35, 214, 31, 485]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D8naxpZ8wwO2",
        "outputId": "293acf00-64d3-4db4-cc97-99b42167efd4"
      },
      "source": [
        "a.decode(ids)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'do you want a car'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hbsy54Uw0wm"
      },
      "source": [
        "import logging\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "import unicodedata\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import datetime\n",
        "import time\n",
        "import re\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "import random"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSoqMipBzVFk"
      },
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "FILE = \"en-fra.txt\"\n",
        "\n",
        "writer = SummaryWriter(\"runs/translation\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "def normalize(s):\n",
        "    return re.sub(' +',' ', \"\".join(c if c in string.ascii_letters else \" \"\n",
        "         for c in unicodedata.normalize('NFD', s.lower().strip())\n",
        "         if  c in string.ascii_letters+\" \"+string.punctuation)).strip()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sHFQZvgzWeW"
      },
      "source": [
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Permet de gérer un vocabulaire.\n",
        "\n",
        "    En test, il est possible qu'un mot ne soit pas dans le\n",
        "    vocabulaire : dans ce cas le token \"__OOV__\" est utilisé.\n",
        "    Attention : il faut tenir compte de cela lors de l'apprentissage !\n",
        "\n",
        "    Utilisation:\n",
        "\n",
        "    - en train, utiliser v.get(\"blah\", adding=True) pour que le mot soit ajouté\n",
        "      automatiquement\n",
        "    - en test, utiliser v[\"blah\"] pour récupérer l'ID du mot (ou l'ID de OOV)\n",
        "    \"\"\"\n",
        "    PAD = 0\n",
        "    EOS = 1\n",
        "    SOS = 2\n",
        "    OOVID = 3\n",
        "\n",
        "    def __init__(self, oov: bool):\n",
        "        self.oov = oov\n",
        "        self.id2word = [\"PAD\", \"EOS\", \"SOS\"]\n",
        "        self.word2id = {\"PAD\": Vocabulary.PAD, \"EOS\": Vocabulary.EOS, \"SOS\": Vocabulary.SOS}\n",
        "        if oov:\n",
        "            self.word2id[\"__OOV__\"] = Vocabulary.OOVID\n",
        "            self.id2word.append(\"__OOV__\")\n",
        "\n",
        "    def __getitem__(self, word: str):\n",
        "        if self.oov:\n",
        "            return self.word2id.get(word, Vocabulary.OOVID)\n",
        "        return self.word2id[word]\n",
        "\n",
        "    def get(self, word: str, adding=True):\n",
        "        try:\n",
        "            return self.word2id[word]\n",
        "        except KeyError:\n",
        "            if adding:\n",
        "                wordid = len(self.id2word)\n",
        "                self.word2id[word] = wordid\n",
        "                self.id2word.append(word)\n",
        "                return wordid\n",
        "            if self.oov:\n",
        "                return Vocabulary.OOVID\n",
        "            raise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.id2word)\n",
        "\n",
        "    def getword(self, idx: int):\n",
        "        if idx < len(self):\n",
        "            return self.id2word[idx]\n",
        "        return None\n",
        "\n",
        "    def getwords(self, idx: List[int]):\n",
        "        return [self.getword(i) for i in idx]\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUoC4MKPzbYI"
      },
      "source": [
        "class TradDataset():\n",
        "    def __init__(self,data,vocOrig,vocDest,adding=True,max_len=10):\n",
        "        self.sentences =[]\n",
        "        for s in tqdm(data.split(\"\\n\")):\n",
        "            if len(s)<1:continue\n",
        "            orig,dest=map(normalize,s.split(\"\\t\")[:2])\n",
        "            if len(orig)>max_len: continue\n",
        "            self.sentences.append((torch.tensor([vocOrig.get(o) for o in a.encode(orig)]+[Vocabulary.EOS]),\\\n",
        "                                   torch.tensor([vocDest.get(o) for o in a.encode(dest)]+[Vocabulary.EOS])))\n",
        "            \n",
        "            #self.sentences.append((torch.tensor([vocOrig.get(o) for o in orig.split(\" \")]+[Vocabulary.EOS]),\\\n",
        "            #                       torch.tensor([vocDest.get(o) for o in dest.split(\" \")]+[Vocabulary.EOS])))\n",
        "    def __len__(self):return len(self.sentences)\n",
        "    def __getitem__(self,i): return self.sentences[i]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBHHpcIUz_Oy"
      },
      "source": [
        "def collate(batch):\n",
        "    orig,dest = zip(*batch)\n",
        "    o_len = torch.tensor([len(o) for o in orig])\n",
        "    d_len = torch.tensor([len(d) for d in dest])\n",
        "    return pad_sequence(orig),o_len,pad_sequence(dest),d_len"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkgZsC9q0BMR"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSMwq5ZU0Ep6",
        "outputId": "46c3e62c-d3b3-497d-e500-21c48f2989cf"
      },
      "source": [
        "with open(FILE) as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "lines = [lines[x] for x in torch.randperm(len(lines))]\n",
        "idxTrain = int(0.8*len(lines))\n",
        "\n",
        "vocEng = Vocabulary(True)\n",
        "vocFra = Vocabulary(True)\n",
        "MAX_LEN=25\n",
        "BATCH_SIZE=64\n",
        "\n",
        "datatrain = TradDataset(\"\".join(lines[:idxTrain]),vocEng,vocFra,max_len=MAX_LEN)\n",
        "datatest = TradDataset(\"\".join(lines[idxTrain:]),vocEng,vocFra,max_len=MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(datatrain, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(datatest, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 136521/136521 [00:13<00:00, 10194.73it/s]\n",
            "100%|██████████| 34132/34132 [00:03<00:00, 10245.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTw9qMkp0GAr"
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "INPUT_SIZE = len(vocEng)\n",
        "OUTPUT_SIZE = len(vocFra)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tBCieET0JFy"
      },
      "source": [
        "class State:\n",
        "    def __init__(self, encoder, decoder, optim_enc, optim_dec):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.optimizer_enc = optim_enc\n",
        "        self.optimizer_dec = optim_dec\n",
        "        self.epoch, self.iteration = 0, 0"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW6baSqu0KN7"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, dropout=0.2)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.embedding(input)\n",
        "        output = embedded\n",
        "        \n",
        "        _, hidden = self.gru(output)\n",
        "        return hidden\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuzU3MGy0Lhj"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrwfbOqz0NEi"
      },
      "source": [
        "def train_loop(dataloader,state,L):\n",
        "    train_loss = 0\n",
        "    for batch, (X, X_sizes, y, y_sizes) in enumerate(dataloader):\n",
        "        \n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        \n",
        "        \n",
        "        input_length = X.size(0)\n",
        "        target_length = y.size(0)\n",
        "        \n",
        "        loss = 0\n",
        "        context = state.encoder(X)\n",
        "\n",
        "        decoder_input = torch.ones(1, X.size(1), dtype=torch.long, device=device) * 2 # 2 is the SOS Token\n",
        "        decoder_hidden = context\n",
        "\n",
        "        teacher_forcing = True if random.random() < 0.5 else False\n",
        "        decoder_outputs = []\n",
        "        if teacher_forcing:\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden = state.decoder(decoder_input, decoder_hidden)\n",
        "                \n",
        "                decoder_output = decoder_output.view(decoder_output.shape[1], -1)\n",
        "                #loss += L(decoder_output, y[di])\n",
        "                decoder_outputs.append(decoder_output)\n",
        "                decoder_input = y[di].view(1, -1) \n",
        "        else:\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden = state.decoder(decoder_input, decoder_hidden)\n",
        "                decoder_output = decoder_output.view(decoder_output.shape[1], -1)\n",
        "                _, pred = torch.max(decoder_output, 1)\n",
        "                pred = pred.view(1, -1)\n",
        "                decoder_input = pred \n",
        "                #loss += L(decoder_output, y[di])\n",
        "                decoder_outputs.append(decoder_output)\n",
        "\n",
        "        decoder_outputs = torch.stack(decoder_outputs).permute(0, 2, 1)\n",
        "        #print(decoder_outputs.shape, y.shape)\n",
        "        loss += L(decoder_outputs, y)\n",
        "        train_loss += loss\n",
        "        \n",
        "        state.optimizer_enc.zero_grad()\n",
        "        state.optimizer_dec.zero_grad()\n",
        "        loss.backward()\n",
        "        state.optimizer_enc.step()\n",
        "        state.optimizer_dec.step() \n",
        "        \n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    return train_loss.item()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KSifvbQ0Ob7"
      },
      "source": [
        "def test_loop(dataloader,state,L):\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        for batch, (X, X_sizes, y, y_sizes) in enumerate(dataloader):\n",
        "            \n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            \n",
        "            \n",
        "            input_length = X.size(0)\n",
        "            target_length = y.size(0)\n",
        "            \n",
        "            loss = 0\n",
        "            context = state.encoder(X)\n",
        "\n",
        "            decoder_input = torch.ones(1, X.size(1), dtype=torch.long, device=device) * 2 # 2 is the SOS Token\n",
        "            decoder_hidden = context\n",
        "            decoder_outputs = []\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden = state.decoder(decoder_input, decoder_hidden)\n",
        "                decoder_output = decoder_output.view(decoder_output.shape[1], -1)\n",
        "                _, pred = torch.max(decoder_output, 1)\n",
        "\n",
        "                pred = pred.view(1, -1)\n",
        "                decoder_input = pred \n",
        "                decoder_outputs.append(decoder_output)\n",
        "                #loss += L(decoder_output, y[di])\n",
        "\n",
        "            decoder_outputs = torch.stack(decoder_outputs).permute(0, 2, 1)\n",
        "            loss += L(decoder_outputs, y)\n",
        "            test_loss += loss\n",
        "            \n",
        "        test_loss = test_loss / len(dataloader)\n",
        "        return test_loss.item()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_C_whrc0QZq"
      },
      "source": [
        "def train(train_loader, save_path, tensorboard_name, iterations=500):\n",
        "    if save_path.is_file():\n",
        "        with save_path.open('rb') as fp:\n",
        "            state = torch.load(fp, map_location=torch.device(device))\n",
        "    else :\n",
        "        enc = Encoder(INPUT_SIZE, HIDDEN_SIZE).to(device)\n",
        "        dec = Decoder(HIDDEN_SIZE, OUTPUT_SIZE).to(device)\n",
        "        optimizer_enc = torch.optim.Adam(enc.parameters(), lr=0.001)\n",
        "        optimizer_dec = torch.optim.Adam(dec.parameters(), lr=0.001)\n",
        "        state = State(enc, dec, optimizer_enc, optimizer_dec)\n",
        "    for epoch in range(state.epoch, iterations):\n",
        "        loss_train = train_loop(train_loader, state, nn.CrossEntropyLoss())\n",
        "        loss_test = test_loop(test_loader, state, nn.CrossEntropyLoss())\n",
        "        with save_path.open(\"wb\") as fp:\n",
        "            state.epoch = epoch + 1\n",
        "            torch.save(state, fp)\n",
        "        writer.add_scalar(tensorboard_name+'/train', loss_train, epoch)\n",
        "        writer.add_scalar(tensorboard_name+'/test', loss_test, epoch)\n",
        "        print('Epoch: ', epoch, 'Loss train: ',loss_train, 'Loss test: ',loss_test)\n",
        "    print(\"Done!\")\n",
        "    return state.encoder, state.decoder"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjALQ83A0SZr",
        "outputId": "4c023701-c871-4b02-dca1-5bf2bf02cc69"
      },
      "source": [
        "writer = SummaryWriter(\"runs/segmentation\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "savepath = Path('./segmentation.pt')\n",
        "encoder, decoder = train(train_loader, savepath, \"Segmentation\", iterations=50)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Loss train:  2.5818679332733154 Loss test:  2.694697856903076\n",
            "Epoch:  1 Loss train:  2.1269571781158447 Loss test:  2.4664785861968994\n",
            "Epoch:  2 Loss train:  1.9573062658309937 Loss test:  2.3791210651397705\n",
            "Epoch:  3 Loss train:  1.8942962884902954 Loss test:  2.346367597579956\n",
            "Epoch:  4 Loss train:  1.8177053928375244 Loss test:  2.2873775959014893\n",
            "Epoch:  5 Loss train:  1.741733193397522 Loss test:  2.328838348388672\n",
            "Epoch:  6 Loss train:  1.7203929424285889 Loss test:  2.279452323913574\n",
            "Epoch:  7 Loss train:  1.7117818593978882 Loss test:  2.2273752689361572\n",
            "Epoch:  8 Loss train:  1.6407390832901 Loss test:  2.2567150592803955\n",
            "Epoch:  9 Loss train:  1.61990487575531 Loss test:  2.218987464904785\n",
            "Epoch:  10 Loss train:  1.632145643234253 Loss test:  2.1899619102478027\n",
            "Epoch:  11 Loss train:  1.6027554273605347 Loss test:  2.226349115371704\n",
            "Epoch:  12 Loss train:  1.5933877229690552 Loss test:  2.2144806385040283\n",
            "Epoch:  13 Loss train:  1.566565752029419 Loss test:  2.210239887237549\n",
            "Epoch:  14 Loss train:  1.562421441078186 Loss test:  2.184352159500122\n",
            "Epoch:  15 Loss train:  1.5226885080337524 Loss test:  2.1968283653259277\n",
            "Epoch:  16 Loss train:  1.5602545738220215 Loss test:  2.181377410888672\n",
            "Epoch:  17 Loss train:  1.524003505706787 Loss test:  2.1611428260803223\n",
            "Epoch:  18 Loss train:  1.5255452394485474 Loss test:  2.180715560913086\n",
            "Epoch:  19 Loss train:  1.5127511024475098 Loss test:  2.157028913497925\n",
            "Epoch:  20 Loss train:  1.4737471342086792 Loss test:  2.157224655151367\n",
            "Epoch:  21 Loss train:  1.5079433917999268 Loss test:  2.1845338344573975\n",
            "Epoch:  22 Loss train:  1.4426792860031128 Loss test:  2.1770036220550537\n",
            "Epoch:  23 Loss train:  1.4611130952835083 Loss test:  2.192169666290283\n",
            "Epoch:  24 Loss train:  1.4680355787277222 Loss test:  2.1685993671417236\n",
            "Epoch:  25 Loss train:  1.438378930091858 Loss test:  2.1383934020996094\n",
            "Epoch:  26 Loss train:  1.4767149686813354 Loss test:  2.11820912361145\n",
            "Epoch:  27 Loss train:  1.4509917497634888 Loss test:  2.1893866062164307\n",
            "Epoch:  28 Loss train:  1.454681396484375 Loss test:  2.1284096240997314\n",
            "Epoch:  29 Loss train:  1.4118069410324097 Loss test:  2.1930956840515137\n",
            "Epoch:  30 Loss train:  1.434739589691162 Loss test:  2.1377744674682617\n",
            "Epoch:  31 Loss train:  1.4070794582366943 Loss test:  2.1463353633880615\n",
            "Epoch:  32 Loss train:  1.4378962516784668 Loss test:  2.1118216514587402\n",
            "Epoch:  33 Loss train:  1.411526083946228 Loss test:  2.1528706550598145\n",
            "Epoch:  34 Loss train:  1.422635555267334 Loss test:  2.1567013263702393\n",
            "Epoch:  35 Loss train:  1.415481448173523 Loss test:  2.156935214996338\n",
            "Epoch:  36 Loss train:  1.4048413038253784 Loss test:  2.1736388206481934\n",
            "Epoch:  37 Loss train:  1.3985313177108765 Loss test:  2.137998342514038\n",
            "Epoch:  38 Loss train:  1.412387728691101 Loss test:  2.1933069229125977\n",
            "Epoch:  39 Loss train:  1.381058931350708 Loss test:  2.113490581512451\n",
            "Epoch:  40 Loss train:  1.4073575735092163 Loss test:  2.154179573059082\n",
            "Epoch:  41 Loss train:  1.3738605976104736 Loss test:  2.1521196365356445\n",
            "Epoch:  42 Loss train:  1.3944993019104004 Loss test:  2.118079900741577\n",
            "Epoch:  43 Loss train:  1.391073226928711 Loss test:  2.1881589889526367\n",
            "Epoch:  44 Loss train:  1.409990906715393 Loss test:  2.1143274307250977\n",
            "Epoch:  45 Loss train:  1.414498209953308 Loss test:  2.120879650115967\n",
            "Epoch:  46 Loss train:  1.365997076034546 Loss test:  2.2218143939971924\n",
            "Epoch:  47 Loss train:  1.3605859279632568 Loss test:  2.149108409881592\n",
            "Epoch:  48 Loss train:  1.394988775253296 Loss test:  2.108105182647705\n",
            "Epoch:  49 Loss train:  1.3983268737792969 Loss test:  2.1575489044189453\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrg9eRQN0WGt",
        "outputId": "0ddc7c8b-1600-4426-9621-b8345af0ef23"
      },
      "source": [
        "with torch.no_grad():\n",
        "    for batch, (X, X_sizes, y, y_sizes) in enumerate(test_loader): \n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        X = X[:, 0:1]\n",
        "        y = y[:, 0:1]\n",
        "        \n",
        "        preds = []\n",
        "        input_length = X.size(0)\n",
        "\n",
        "        loss = 0\n",
        "        context = encoder(X)\n",
        "\n",
        "        pred = torch.ones(1, X.size(1), dtype=torch.long, device=device) * 2\n",
        "        decoder_hidden = context\n",
        "\n",
        "        while pred.item() != 1: #EOS = 1\n",
        "            decoder_output, decoder_hidden = decoder(pred, decoder_hidden)\n",
        "            decoder_output = decoder_output.view(decoder_output.shape[1], -1)\n",
        "            _, pred = torch.max(decoder_output, 1)\n",
        "            preds.append(pred.item())  \n",
        "            pred = pred.view(1, -1)\n",
        "        break\n",
        "\n",
        "wds = vocEng.getwords(X)\n",
        "wds = a.decode(list(filter(lambda x: type(x) == int, wds)))\n",
        "trs = vocFra.getwords(y)\n",
        "trs = a.decode(list(filter(lambda x: type(x) == int, trs)))\n",
        "predtrs = vocFra.getwords(preds)\n",
        "predtrs = a.decode(list(filter(lambda x: type(x) == int, predtrs)))\n",
        "print(wds)\n",
        "print(trs)\n",
        "print(predtrs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "who did it\n",
            "qui a fait ca\n",
            "qui l a fait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBmywE0I90dQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}