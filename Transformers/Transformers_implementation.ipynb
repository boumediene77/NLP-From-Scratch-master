{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -r http://webia.lip6.fr/~baskiotisn/requirements-amal.txt","metadata":{"id":"wTMGeOMhyVC4","outputId":"2b60140f-35d6-4d1e-d128-8176760cfeb6","execution":{"iopub.status.busy":"2022-01-22T22:32:55.886790Z","iopub.execute_input":"2022-01-22T22:32:55.887424Z","iopub.status.idle":"2022-01-22T22:33:43.626876Z","shell.execute_reply.started":"2022-01-22T22:32:55.887383Z","shell.execute_reply":"2022-01-22T22:33:43.626021Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport click\nfrom torch.utils.tensorboard import SummaryWriter\nimport logging\nimport re\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport numpy as np\nimport time\nfrom datamaestro import prepare_dataset\nimport torch.nn.functional as F\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"id":"Ven94-Thx-vY","execution":{"iopub.status.busy":"2022-01-22T22:36:07.508226Z","iopub.execute_input":"2022-01-22T22:36:07.508815Z","iopub.status.idle":"2022-01-22T22:36:08.279467Z","shell.execute_reply.started":"2022-01-22T22:36:07.508716Z","shell.execute_reply":"2022-01-22T22:36:08.278745Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 500\nlogging.basicConfig(level=logging.INFO)","metadata":{"id":"5dOfYR5FyT27","execution":{"iopub.status.busy":"2022-01-22T22:36:11.502883Z","iopub.execute_input":"2022-01-22T22:36:11.503447Z","iopub.status.idle":"2022-01-22T22:36:11.507092Z","shell.execute_reply.started":"2022-01-22T22:36:11.503412Z","shell.execute_reply":"2022-01-22T22:36:11.506348Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class FolderText(Dataset):\n    \"\"\"Dataset basé sur des dossiers (un par classe) et fichiers\"\"\"\n\n    def __init__(self, classes, folder: Path, tokenizer, load=False):\n        self.tokenizer = tokenizer\n        self.files = []\n        self.filelabels = []\n        self.labels = {}\n        for ix, key in enumerate(classes):\n            self.labels[key] = ix\n\n        for label in classes:\n            for file in (folder / label).glob(\"*.txt\"):\n                self.files.append(file.read_text(encoding='utf-8') if load else file)\n                self.filelabels.append(self.labels[label])\n\n    def __len__(self):\n        return len(self.filelabels)\n\n    def __getitem__(self, ix):\n        s = self.files[ix]\n        return self.tokenizer(s if isinstance(s, str) else s.read_text(encoding='utf-8')), self.filelabels[ix]\n    def get_txt(self,ix):\n        s = self.files[ix]\n        return s if isinstance(s,str) else s.read_text(encoding='utf-8'), self.filelabels[ix]","metadata":{"id":"qI14FWBTyq6G","execution":{"iopub.status.busy":"2022-01-22T22:36:11.792371Z","iopub.execute_input":"2022-01-22T22:36:11.792814Z","iopub.status.idle":"2022-01-22T22:36:11.805409Z","shell.execute_reply.started":"2022-01-22T22:36:11.792785Z","shell.execute_reply":"2022-01-22T22:36:11.804735Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def get_imdb_data(embedding_size=50):\n    \"\"\"Renvoie l'ensemble des donnéees nécessaires pour l'apprentissage (embedding_size = [50,100,200,300])\n\n    - dictionnaire word vers ID\n    - embeddings (Glove)\n    - DataSet (FolderText) train\n    - DataSet (FolderText) test\n\n    \"\"\"\n    WORDS = re.compile(r\"\\S+\")\n\n    words, embeddings = prepare_dataset(\n        'edu.stanford.glove.6b.%d' % embedding_size).load()\n    OOVID = len(words)\n    words.append(\"__OOV__\")\n    word2id = {word: ix for ix, word in enumerate(words)}\n    embeddings = np.vstack((embeddings, np.zeros(embedding_size)))\n\n    def tokenizer(t):\n        return [word2id.get(x, OOVID) for x in re.findall(WORDS, t.lower())]\n\n    logging.info(\"Loading embeddings\")\n\n    logging.info(\"Get the IMDB dataset\")\n    ds = prepare_dataset(\"edu.stanford.aclimdb\")\n\n    return word2id, embeddings, FolderText(ds.train.classes, ds.train.path, tokenizer, load=False), FolderText(ds.test.classes, ds.test.path, tokenizer, load=False)\n","metadata":{"id":"c1Kv1vgsy8FK","execution":{"iopub.status.busy":"2022-01-22T22:36:12.160230Z","iopub.execute_input":"2022-01-22T22:36:12.160989Z","iopub.status.idle":"2022-01-22T22:36:12.168592Z","shell.execute_reply.started":"2022-01-22T22:36:12.160943Z","shell.execute_reply":"2022-01-22T22:36:12.167915Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"epochs = 10\ntest_iterations = 10\nmodeltype = 1\nemb_size = 100\nbatch_size = 64","metadata":{"id":"WNY4yIZizTZ9","execution":{"iopub.status.busy":"2022-01-22T22:36:13.150759Z","iopub.execute_input":"2022-01-22T22:36:13.151201Z","iopub.status.idle":"2022-01-22T22:36:13.155536Z","shell.execute_reply.started":"2022-01-22T22:36:13.151157Z","shell.execute_reply":"2022-01-22T22:36:13.154877Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nword2id, embeddings, train_data, test_data = get_imdb_data(emb_size)\nid2word = dict((v, k) for k, v in word2id.items())\nPAD = word2id[\"__OOV__\"]\nembeddings = torch.Tensor(embeddings)\nemb_layer = nn.Embedding.from_pretrained(torch.Tensor(embeddings))\n\ndef collate(batch):\n    \"\"\" Collate function for DataLoader \"\"\"\n    data = [torch.LongTensor(item[0][:MAX_LENGTH]) for item in batch]\n    lens = [len(d) for d in data]\n    labels = [item[1] for item in batch]\n    return emb_layer(torch.nn.utils.rnn.pad_sequence(data, batch_first=True,padding_value = PAD)).to(device), torch.LongTensor(labels).to(device), torch.Tensor(lens).to(device)\n\n\ntrain_loader = DataLoader(train_data, shuffle=True,\n                      batch_size=batch_size, collate_fn=collate)\ntest_loader = DataLoader(test_data, batch_size=batch_size,collate_fn=collate,shuffle=False)","metadata":{"id":"4Xy7_7G1zVGM","outputId":"1ea2e5c0-2bd3-441a-a182-c6df12b67a9f","execution":{"iopub.status.busy":"2022-01-22T22:36:14.887041Z","iopub.execute_input":"2022-01-22T22:36:14.887737Z","iopub.status.idle":"2022-01-22T22:41:37.181314Z","shell.execute_reply.started":"2022-01-22T22:36:14.887702Z","shell.execute_reply":"2022-01-22T22:41:37.180567Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class SelfAttention(nn.Module):\n    def __init__(self, emb_size):\n        super(SelfAttention, self).__init__()\n        self.emb_size = emb_size\n        self.out_size1 = 100\n        \n        self.q1 = nn.Linear(self.emb_size, self.emb_size)\n        self.k1 = nn.Linear(self.emb_size, self.emb_size)\n        self.v1 = nn.Linear(self.emb_size, self.emb_size)\n        self.l1 = nn.Linear(self.emb_size, self.emb_size)\n        \n    def forward(self, X, d):\n        q = self.q1(X)\n        k = self.k1(X)\n        v = self.v1(X)\n\n        a = (torch.bmm(q, k.transpose(-2, -1)))\n        b = (1/d**(-1/2))\n        b = b.reshape(-1, 1, 1)\n        b = b.expand(-1 ,a.shape[1], a.shape[2])\n        logalpha =  b * a \n        \n        mask0 = torch.zeros_like(logalpha)\n        logalpha_masked = torch.where(logalpha.float()==0, mask0-np.inf, logalpha)\n        \n        alpha = F.softmax(logalpha_masked, dim=-1)\n        \n        f = torch.bmm(alpha, v)\n        g = F.relu(self.l1(f))\n        \n        return g","metadata":{"id":"N3cBrrPGzWqZ","execution":{"iopub.status.busy":"2022-01-22T22:42:37.183546Z","iopub.execute_input":"2022-01-22T22:42:37.184544Z","iopub.status.idle":"2022-01-22T22:42:37.198469Z","shell.execute_reply.started":"2022-01-22T22:42:37.184496Z","shell.execute_reply":"2022-01-22T22:42:37.197794Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, emb_size, L):\n        super(Transformer, self).__init__()\n        self.attentions = nn.ModuleList()\n        self.fc = nn.Linear(emb_size, 1)\n        for _ in range(L):\n            self.attentions.append(SelfAttention(emb_size))\n            \n    def forward(self, x, d):\n        for i, l in enumerate(self.attentions):\n            x = l(x, d)\n        mean = torch.mean(x, dim=1)\n        y = self.fc(mean)\n        return F.sigmoid(y)","metadata":{"id":"20yV_eAxzmbw","execution":{"iopub.status.busy":"2022-01-22T22:42:37.468720Z","iopub.execute_input":"2022-01-22T22:42:37.469005Z","iopub.status.idle":"2022-01-22T22:42:37.481725Z","shell.execute_reply.started":"2022-01-22T22:42:37.468961Z","shell.execute_reply":"2022-01-22T22:42:37.480737Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class State:\n    def __init__(self, model, optim):\n        self.model = model\n        self.optimizer = optim\n        self.epoch, self.iteration = 0, 0","metadata":{"id":"ShX8LXrbznz3","execution":{"iopub.status.busy":"2022-01-22T22:42:38.054439Z","iopub.execute_input":"2022-01-22T22:42:38.055109Z","iopub.status.idle":"2022-01-22T22:42:38.061203Z","shell.execute_reply.started":"2022-01-22T22:42:38.055074Z","shell.execute_reply":"2022-01-22T22:42:38.059112Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, state):\n    train_loss = 0\n    train_accuracy = 0\n    L = nn.BCELoss()\n    for batch, (X, y, d) in enumerate(dataloader):    \n        yhat = state.model(X, d)\n        y = y.reshape(-1, 1).float()\n        loss = L(yhat, y)\n        state.optimizer.zero_grad()\n        loss.backward()\n        state.optimizer.step()\n        state.iteration += 1\n\n        train_loss += loss\n        acc = (torch.sum( torch.where(yhat > 0.5, 1, 0) == y) / dataloader.batch_size)\n        train_accuracy += acc\n\n    train_loss = train_loss / len(dataloader)\n    train_accuracy = train_accuracy / len(dataloader)\n    return train_loss.item(), train_accuracy.item()","metadata":{"id":"39dSTAW-zpGg","execution":{"iopub.status.busy":"2022-01-22T22:42:38.550671Z","iopub.execute_input":"2022-01-22T22:42:38.551191Z","iopub.status.idle":"2022-01-22T22:42:38.558202Z","shell.execute_reply.started":"2022-01-22T22:42:38.551154Z","shell.execute_reply":"2022-01-22T22:42:38.557526Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def test_loop(dataloader, model):\n    test_loss = 0\n    test_accuracy = 0\n    L = nn.BCELoss()\n    for batch, (X, y, d) in enumerate(dataloader):  \n        with torch.no_grad():\n            yhat = model(X, d)\n            y = y.reshape(-1, 1).float()\n            loss = L(yhat, y)\n            test_loss += loss\n            acc = (torch.sum( torch.where(yhat > 0.5, 1, 0) == y) / dataloader.batch_size)\n            test_accuracy += acc\n\n    test_loss = test_loss / len(dataloader)\n    test_accuracy = test_accuracy / len(dataloader)\n    return test_loss.item(), test_accuracy.item()","metadata":{"execution":{"iopub.status.busy":"2022-01-22T22:42:39.052493Z","iopub.execute_input":"2022-01-22T22:42:39.052994Z","iopub.status.idle":"2022-01-22T22:42:39.060324Z","shell.execute_reply.started":"2022-01-22T22:42:39.052959Z","shell.execute_reply":"2022-01-22T22:42:39.059596Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train(data_train, data_test, save_path, Model, tensorboard_name, iterations=500):\n    if save_path.is_file():\n        with save_path.open('rb') as fp:\n            state = torch.load(fp)\n    else :\n        model = Model(100,3).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n        state = State(model, optimizer)\n    for epoch in range(state.epoch, iterations):\n        loss_train, acc_train = train_loop(data_train, state)\n        loss_test, acc_test = test_loop(data_test, state.model)\n        with save_path.open(\"wb\") as fp:\n            state.epoch = epoch + 1\n            torch.save(state, fp)\n        #loss_test = test_loop(data_test, state.model)\n        #writer.add_scalar(tensorboard_name+'/train', loss_test, epoch)\n        #writer.add_scalar(tensorboard_name+'/dev', loss_train, epoch)\n        print('Epoch: ', epoch, 'Loss train: ',loss_train, 'Acc train: ',acc_train)\n        print('       ', epoch, 'Loss test: ',loss_test, 'Acc test: ',acc_test)\n    print(\"Done!\")\n    return state.model","metadata":{"id":"euLsMpTDzqcc","execution":{"iopub.status.busy":"2022-01-22T22:42:40.664850Z","iopub.execute_input":"2022-01-22T22:42:40.665564Z","iopub.status.idle":"2022-01-22T22:42:40.673527Z","shell.execute_reply.started":"2022-01-22T22:42:40.665527Z","shell.execute_reply":"2022-01-22T22:42:40.672339Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"savepath = Path('./Transformer2.pt')\nmodel = train(train_loader, test_loader, savepath, Transformer, \"baseline\", iterations=20)","metadata":{"id":"_ofX2c_X0vrN","outputId":"8807cee4-92e7-42f4-f340-adcf7d03d3f0","execution":{"iopub.status.busy":"2022-01-22T22:42:42.738700Z","iopub.execute_input":"2022-01-22T22:42:42.739419Z","iopub.status.idle":"2022-01-22T22:49:22.837281Z","shell.execute_reply.started":"2022-01-22T22:42:42.739382Z","shell.execute_reply":"2022-01-22T22:49:22.836445Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class SelfAttentionResidual(nn.Module):\n    def __init__(self, emb_size):\n        super(SelfAttentionResidual, self).__init__()\n        self.emb_size = emb_size\n        self.out_size1 = 100\n        \n        self.q1 = nn.Linear(self.emb_size, self.emb_size)\n        self.k1 = nn.Linear(self.emb_size, self.emb_size)\n        self.v1 = nn.Linear(self.emb_size, self.emb_size)\n        self.l1 = nn.Linear(self.emb_size, self.emb_size)\n        \n    def forward(self, X, d):\n        X_copy = X\n        q = self.q1(X)\n        \n        k = self.k1(X)\n        v = self.v1(X)\n        print(v.shape)\n        a = (torch.bmm(q, k.transpose(-2, -1)))\n        b = (1/d**(-1/2))\n        b = b.reshape(-1, 1, 1)\n        b = b.expand(-1 ,a.shape[1], a.shape[2])\n        logalpha =  b * a \n        \n        mask0 = torch.zeros_like(logalpha)\n        logalpha_masked = torch.where(logalpha.float()==0, mask0-np.inf, logalpha)\n        print(logalpha_masked.shape)\n        alpha = F.softmax(logalpha_masked, dim=-1)\n        print(alpha.shape, v.shape)\n        f = torch.bmm(alpha, v)\n        print(f.shape)\n        f = f + X_copy\n        \n        g = F.relu(self.l1(f))\n        print(g.shape)\n        return g","metadata":{"execution":{"iopub.status.busy":"2022-01-23T00:19:59.506858Z","iopub.execute_input":"2022-01-23T00:19:59.507632Z","iopub.status.idle":"2022-01-23T00:19:59.519906Z","shell.execute_reply.started":"2022-01-23T00:19:59.507595Z","shell.execute_reply":"2022-01-23T00:19:59.519242Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class TransformerResidual(nn.Module):\n    def __init__(self, emb_size, L):\n        super(TransformerResidual, self).__init__()\n        self.attentions = nn.ModuleList()\n        self.norms = nn.ModuleList()\n        self.fc = nn.Linear(emb_size, 1)\n        for _ in range(L):\n            self.attentions.append(SelfAttentionResidual(emb_size))\n            self.norms.append(nn.LayerNorm(emb_size))\n            \n    def forward(self, x, d):\n        for i, l in enumerate(self.attentions):\n            x = l(x, d)\n            if i != len(self.attentions) -1:\n                x = self.norms[i](x)\n        mean = torch.mean(x, dim=1)\n        y = self.fc(mean)\n        return F.sigmoid(y)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T22:54:03.551615Z","iopub.execute_input":"2022-01-22T22:54:03.552044Z","iopub.status.idle":"2022-01-22T22:54:03.559848Z","shell.execute_reply.started":"2022-01-22T22:54:03.552007Z","shell.execute_reply":"2022-01-22T22:54:03.558953Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"savepath = Path('./Transformer_res.pt')\nmodel = train(train_loader, test_loader, savepath, TransformerResidual, \"baseline\", iterations=20)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T22:54:03.973535Z","iopub.execute_input":"2022-01-22T22:54:03.974167Z","iopub.status.idle":"2022-01-22T23:01:21.633562Z","shell.execute_reply.started":"2022-01-22T22:54:03.974132Z","shell.execute_reply":"2022-01-22T23:01:21.632819Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import math","metadata":{"execution":{"iopub.status.busy":"2022-01-22T22:49:22.838966Z","iopub.execute_input":"2022-01-22T22:49:22.839209Z","iopub.status.idle":"2022-01-22T22:49:22.844123Z","shell.execute_reply.started":"2022-01-22T22:49:22.839175Z","shell.execute_reply":"2022-01-22T22:49:22.843421Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"Position embeddings\"\n\n    def __init__(self, d_model: int, max_len: int = 5000):\n        \"\"\"Génère des embeddings de position\n\n        Args:\n            d_model (int): Dimension des embeddings à générer\n            max_len (int, optional): Longueur maximale des textes.\n                Attention, plus cette valeur est haute, moins bons seront les embeddings de position.\n        \"\"\"\n        super().__init__()\n\n        pe = torch.zeros(max_len, d_model, dtype=torch.float)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) *\n                             -(math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        pe.requires_grad = False\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"Ajoute les embeddings de position\"\"\"\n        x = x + self.pe[:, :x.size(1)]\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-01-22T22:49:22.845647Z","iopub.execute_input":"2022-01-22T22:49:22.846178Z","iopub.status.idle":"2022-01-22T22:49:22.856292Z","shell.execute_reply.started":"2022-01-22T22:49:22.846143Z","shell.execute_reply":"2022-01-22T22:49:22.855630Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"pos_encoding = PositionalEncoding(100, max_len = 500)\nimport seaborn as sns\nsns.set()\nml = MAX_LENGTH\nheatmap = np.ndarray((ml,ml))\nfor i in range(ml):\n    for j in range(ml):\n        pei = pos_encoding.pe[0,i,:]\n        pej = pos_encoding.pe[0,j,:]\n        heatmap[i, j] = torch.dot(pei, pej).item()\nsns.heatmap(heatmap)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T22:49:22.858187Z","iopub.execute_input":"2022-01-22T22:49:22.858602Z","iopub.status.idle":"2022-01-22T22:49:29.028517Z","shell.execute_reply.started":"2022-01-22T22:49:22.858566Z","shell.execute_reply":"2022-01-22T22:49:29.027791Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class TransformerPosEncoding(nn.Module):\n    def __init__(self, emb_size, L):\n        super(TransformerPosEncoding, self).__init__()\n        self.attentions = nn.ModuleList()\n        self.fc = nn.Linear(emb_size, 1)\n        self.pos_encoding = PositionalEncoding(emb_size, max_len = 500)\n        for _ in range(L):\n            self.attentions.append(SelfAttention(emb_size))\n            \n    def forward(self, x, d):\n        x = self.pos_encoding(x)\n        for i, l in enumerate(self.attentions):\n            x = l(x, d)\n            \n        mean = torch.mean(x, dim=1)\n        y = self.fc(mean)\n        return F.sigmoid(y)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T20:01:42.748450Z","iopub.execute_input":"2022-01-22T20:01:42.748710Z","iopub.status.idle":"2022-01-22T20:01:42.756563Z","shell.execute_reply.started":"2022-01-22T20:01:42.748684Z","shell.execute_reply":"2022-01-22T20:01:42.755677Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"savepath = Path('./Transformer_pos2.pt')\nmodel = train(train_loader, test_loader, savepath, TransformerPosEncoding, \"baseline\", iterations=20)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T20:01:54.143120Z","iopub.execute_input":"2022-01-22T20:01:54.143828Z","iopub.status.idle":"2022-01-22T20:08:03.172227Z","shell.execute_reply.started":"2022-01-22T20:01:54.143787Z","shell.execute_reply":"2022-01-22T20:08:03.171290Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"class TransformerCLS(nn.Module):\n    def __init__(self, emb_size, L):\n        super(TransformerCLS, self).__init__()\n        self.attentions = nn.ModuleList()\n        self.fc = nn.Linear(emb_size, 1)\n        self.cls = torch.nn.Parameter(torch.ones(emb_size))\n        for _ in range(L):\n            self.attentions.append(SelfAttention(emb_size))\n            \n    def forward(self, x, d):\n        x = torch.cat((self.cls.unsqueeze(0).repeat((x.shape[0],1,1)),x),dim=1)\n        for i, l in enumerate(self.attentions):\n            x = l(x, d)\n        mean = torch.mean(x, dim=1)\n        y = self.fc(mean)\n        return F.sigmoid(y)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:04:19.886586Z","iopub.execute_input":"2022-01-22T19:04:19.887177Z","iopub.status.idle":"2022-01-22T19:04:19.894841Z","shell.execute_reply.started":"2022-01-22T19:04:19.887137Z","shell.execute_reply":"2022-01-22T19:04:19.893864Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"savepath = Path('./Transformer_cls.pt')\nmodel = train(train_loader, test_loader, savepath, TransformerCLS, \"baseline\", iterations=20)","metadata":{"execution":{"iopub.status.busy":"2022-01-22T19:04:21.225358Z","iopub.execute_input":"2022-01-22T19:04:21.226140Z","iopub.status.idle":"2022-01-22T19:10:33.914871Z","shell.execute_reply.started":"2022-01-22T19:04:21.226102Z","shell.execute_reply":"2022-01-22T19:10:33.914061Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}